{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions (or axes) of our tensors usually index something like pixel locations or color channels. This means when we want to index into a tensor, we need to remember the ordering of the dimensions and write our indexing accordingly. As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone. To make things concrete, imagine that we have a 3D tensor like img_t from previous exercise, and we want to convert it to grayscale. We looked up typical weights for the colors to derive a single brightness value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "img_t = torch.randn(3, 5, 5) # shape [channels, row, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5589, -1.0746, -0.6100, -0.8683, -1.6213],\n",
       "         [ 0.5417, -0.1866,  0.4269, -0.7971, -0.3946],\n",
       "         [-0.6690, -0.3845, -1.7485,  0.1154,  0.9909],\n",
       "         [-0.0973, -0.1809, -1.3386,  0.2153, -0.8393],\n",
       "         [-2.1066, -0.5012,  0.1529, -0.9267,  0.0737]],\n",
       "\n",
       "        [[-1.1205, -0.9946,  0.4220,  0.6887,  0.0620],\n",
       "         [-0.3219,  0.9497, -0.2740, -1.4052, -0.3920],\n",
       "         [-0.0355, -1.2609, -0.3999, -0.3529, -1.7494],\n",
       "         [-1.5558,  0.3576, -0.1922, -0.6631,  0.1494],\n",
       "         [-0.1353, -0.4318,  1.6777, -1.0682, -0.1450]],\n",
       "\n",
       "        [[ 0.0434,  1.8187,  0.3129,  0.6049, -0.0367],\n",
       "         [ 1.0821, -1.0859, -0.0346, -0.4126, -2.1605],\n",
       "         [-0.2485,  0.0835,  0.1817, -0.6669,  2.2044],\n",
       "         [-0.0636,  0.5213, -0.0782,  0.1915, -0.7568],\n",
       "         [-0.0136,  1.1733, -0.5161, -1.7136, -1.2537]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also often want our code to generalize for example, from grayscale images represented as 2D tensors with height and width dimensions to color images adding a third channel dimension (as in RGB), or from a single image to a batch of images. In previous exercise (`notebook 2a`), we introduced an additional batch dimension in `batch_t` here we pretend to have a batch of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8129, -0.1675, -0.8787, -0.3952,  1.9291],\n",
       "          [-0.8607,  0.3021,  1.7827,  0.0149,  0.4058],\n",
       "          [-0.1087,  0.3600,  0.2970,  1.6537,  0.0348],\n",
       "          [-1.8040, -2.7514,  0.5948, -1.2703,  1.0410],\n",
       "          [-0.4613, -0.8717, -0.4430, -1.0025, -1.1186]],\n",
       "\n",
       "         [[ 0.6857, -0.8334,  0.1545, -0.7820, -0.6724],\n",
       "          [-1.9802, -0.9720,  0.7111,  0.6594,  0.4160],\n",
       "          [-0.2974,  1.5287, -0.4158,  1.2487, -1.2285],\n",
       "          [ 0.3075,  0.6630, -2.3555, -0.7474,  0.0067],\n",
       "          [ 0.7398,  0.3044,  0.3735, -1.1308,  0.9371]],\n",
       "\n",
       "         [[ 0.6127, -0.7907, -2.1932,  0.1061, -0.3831],\n",
       "          [-0.3182,  1.6606, -0.0240,  1.2927,  0.1177],\n",
       "          [ 0.8799, -0.2720,  0.6009, -0.0716, -1.0625],\n",
       "          [-0.3228,  0.6162,  0.0630,  1.8843, -0.0269],\n",
       "          [ 0.2552,  0.8008,  1.6354,  0.1947,  0.1643]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7715, -0.1015,  1.0379, -1.6435,  0.7330],\n",
       "          [ 1.1886,  0.3734, -1.0309, -1.1605,  0.4632],\n",
       "          [ 0.9654, -0.2278, -0.1470, -0.3800,  0.0524],\n",
       "          [ 1.2127, -0.1610, -0.9226, -1.2481, -0.8991],\n",
       "          [-0.0786, -0.4734,  0.2852,  0.5469,  0.9325]],\n",
       "\n",
       "         [[ 1.3331,  0.2121,  0.3556,  0.0041,  0.8953],\n",
       "          [-1.1890,  0.7148, -0.0450,  1.4163,  0.4393],\n",
       "          [-0.8878,  2.5398,  1.0144, -0.9345,  0.1165],\n",
       "          [-0.4445, -1.0392,  0.8357,  0.7709,  1.6799],\n",
       "          [-0.2034, -2.5809, -0.5676, -0.4342,  0.3021]],\n",
       "\n",
       "         [[-0.0165,  0.5266,  0.3813, -0.0449,  0.4188],\n",
       "          [-0.4345, -1.0666, -1.6260,  1.5959,  0.4519],\n",
       "          [ 0.0662, -0.1965, -0.4495,  0.3671, -2.6779],\n",
       "          [-1.7500,  1.3813, -0.6827, -0.3081, -0.0625],\n",
       "          [-0.6730,  0.2819, -0.4943,  0.5065,  1.7805]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, column]\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5453, -0.0835,  0.0416,  0.1417, -0.5320],\n",
       "        [ 0.4340, -0.1076,  0.0394, -0.8716, -0.9824],\n",
       "        [-0.3177, -0.5206, -0.6556, -0.3015,  0.4820],\n",
       "        [-0.5722,  0.2327, -0.5363, -0.0854, -0.4822],\n",
       "        [-0.7518,  0.0801,  0.4382, -1.2362, -0.4417]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3) # mean from all channels\n",
    "img_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1619, -0.5972, -0.9725, -0.3570,  0.2912],\n",
       "         [-1.0530,  0.3302,  0.8233,  0.6557,  0.3132],\n",
       "         [ 0.1579,  0.5389,  0.1607,  0.9436, -0.7521],\n",
       "         [-0.6064, -0.4907, -0.5659, -0.0445,  0.3403],\n",
       "         [ 0.1779,  0.0778,  0.5220, -0.6462, -0.0058]],\n",
       "\n",
       "        [[ 1.0293,  0.2124,  0.5916, -0.5614,  0.6824],\n",
       "         [-0.1450,  0.0072, -0.9007,  0.6172,  0.4515],\n",
       "         [ 0.0479,  0.7052,  0.1393, -0.3158, -0.8363],\n",
       "         [-0.3273,  0.0604, -0.2565, -0.2618,  0.2394],\n",
       "         [-0.3183, -0.9241, -0.2589,  0.2064,  1.0050]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive = batch_t.mean(-3) # mean all 3 channels 2 batch, resulting 2 channels bcs before its 2 batch\n",
    "batch_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have the weight, too. PyTorch will allow us to multiply things that are the same shape, as well as shapes where one operand is of size 1 in a given dimension. It also appends leading dimensions of size 1 automatically. This is a feature called broadcasting. `batch_t` of shape (2, 3, 5, 5) is multiplied by `unsqueezed_weights` of shape (3, 1, 1), resulting in a tensor of shape (2, 3, 5, 5), from which we can then sum the third dimension from the end (the three channels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "\n",
    "img_weights.shape, batch_weights.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Tensor dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allocate a tensor of the right numeric type, we can specify the proper dtype as an argument to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_points = torch.ones(10, 2, dtype=torch.double)\n",
    "double_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n",
    "short_points.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also cast the output of a tensor creation function to the right type using the corresponding casting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_points = torch.ones(10, 2).double()\n",
    "double_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points = torch.ones(10, 2).short()\n",
    "short_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more convinience method\n",
    "double_points = torch.zeros(10, 2).to(torch.double)\n",
    "short_points = torch.ones(10, 2).to(dtype=torch.short)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When mixing input types in operations, the inputs are converted to the larger type automatically. Thus, if we want 32-bit computation, we need to make sure all our inputs are (at most) 32-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3175, 0.5838, 1.2843, 1.3425, 0.5230], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.randn(5, dtype=torch.double)\n",
    "points_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 0], dtype=torch.int16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_short = points_64.to(torch.short)\n",
    "points_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 1.2843, 1.3425, 0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 * points_short"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result show `int16` converted to larger type, which is `float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Nov 29 2022, 20:00:25) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
