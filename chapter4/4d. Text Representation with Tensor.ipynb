{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text into number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two particularly intuitive levels at which networks operate on text: at the\n",
    "character level, by processing one character at a time, and at the word level, where\n",
    "individual words are the finest-grained entities to be seen by the network. The technique with which we encode text information into tensor form is the same whether we operate at the character level or the word level. And it’s not magic, either. We stumbled upon it earlier: one-hot encoding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with a character-level example. First, let’s get some text to process. An\n",
    "amazing resource here is Project Gutenberg (www.gutenberg.org), a volunteer effort\n",
    "to digitize and archive cultural work and make it available for free in open formats,\n",
    "including plain text files. If we’re aiming at larger-scale corpora, the Wikipedia corpus\n",
    "stands out: it’s the complete collection of Wikipedia articles, containing 1.9 billion\n",
    "words and more than 4.4 million articles. Several other corpora can be found at the\n",
    "English Corpora website (www.english-corpora.org).\n",
    "Let’s load Jane Austen’s Pride and Prejudice from the Project Gutenberg website:\n",
    "www.gutenberg.org/files/1342/1342-0.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/p1ch4/jane-austen/1342-0.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding Character"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s one more detail we need to take care of before we proceed: encoding. This is\n",
    "a pretty vast subject, and we will just touch on it. Every written character is represented\n",
    "by a code: a sequence of bits of appropriate length so that each character can be\n",
    "uniquely identified. The simplest such encoding is ASCII (American Standard Code\n",
    "for Information Interchange), which dates back to the 1960s. ASCII encodes 128 characters using 128 integers. For instance, the letter a corresponds to binary 1100001 or decimal 97, the letter b to binary 1100010 or decimal 98, and so on. The encoding fits 8 bits, which was a big bonus in 1965."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to one-hot encode our characters. It is instrumental to limit the one-hot\n",
    "encoding to a character set that is useful for the text being analyzed. In our case, since\n",
    "we loaded text in English, it is safe to use ASCII and deal with a small encoding. We\n",
    "could also make all of the characters lowercase, to reduce the number of different\n",
    "characters in our encoding. Similarly, we could screen out punctuation, numbers, or\n",
    "other characters that aren’t relevant to our expected kinds of text. This may or may\n",
    "not make a practical difference to a neural network, depending on the task at hand.\n",
    "\n",
    "\n",
    "At this point, we need to parse through the characters in the text and provide a\n",
    "one-hot encoding for each of them. Each character will be represented by a vector of\n",
    "length equal to the number of different characters in the encoding. This vector will\n",
    "contain all zeros except a one at the index corresponding to the location of the char-\n",
    "acter in the encoding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split our text into a list of lines and pick an arbitrary line to focus on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split(\"\\n\")\n",
    "line = lines[200]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "letter_t = torch.zeros(len(line), 128)\n",
    "letter_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `letter_t` holds a one-hot-encoded character per row. Now we just have to\n",
    "set a one on each row in the correct position so that each row represents the correct\n",
    "character. The index where the one has to be set corresponds to the index of the character in the encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    letter_t[i][letter_index] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
