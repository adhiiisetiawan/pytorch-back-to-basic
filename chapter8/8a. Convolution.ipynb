{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img.mean(0), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAp+klEQVR4nO3dfWyV93nG8csY+/j9GGNs42IokBSa8jKNJdRKy2jweJkUkQZNSVtppIsSJTPREtalZWqTJtvkLJXatBUlfyyDVSqhzVQSJVrJElKMugEbLIgmXT1A7oCBDZj43T429rM/Krw54eW5jA8/23w/0pGCfXP79zy/55w7B59znYwoiiIBAHCDTQq9AADAzYkBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIYnLoBXzY4OCgTp8+rcLCQmVkZIReDgDAFEWROjo6VFlZqUmTrvw8Z8wNoNOnT6uqqir0MgAA1+nkyZOaMWPGFb+ftgG0efNmfetb31JTU5MWL16s73//+7rjjjuu+fcKCwslSU8++aQSiUSsn5VKpWKvq7OzM3atJOXk5MSuvbT2uPLy8mLXJpNJq/eUKVPSsg5J6u/vt+q7urpi1/b09Fi9x4qr/V/e5WRlZaWtf3Z2ttXbqXfXffHixdi1HR0dVm/nunIeIyRpYGDAqnfuE93d3VZv5z7R29tr9XbqMzMzY9f29fXppZdeuuZjYloG0I9//GNt3LhRL774opYuXaoXXnhBq1atUkNDg8rKyq76dy/9s1sikbAe/ONyHzzjDkHJG1aSlJubm5ZayRsq+fn5Vm/3HN4McYPOnVMaWwPIucbTOYDcB/3BwcHYte7/ILhrcfbfWbdb797XnN6TJ/vj4lq/RknLixC+/e1v66GHHtKXv/xl3XbbbXrxxReVl5env//7v0/HjwMAjEOjPoD6+vp06NAh1dTU/N8PmTRJNTU12rdv30fqU6mU2tvbh90AABPfqA+g8+fPa2BgQOXl5cO+Xl5erqampo/U19XVKZlMDt14AQIA3ByCvw9o06ZNamtrG7qdPHky9JIAADfAqL8IobS0VJmZmWpubh729ebmZlVUVHykPpFIWL8EBQBMDKP+DCg7O1tLlizR7t27h742ODio3bt3q7q6erR/HABgnErLy7A3btyo9evX6/d+7/d0xx136IUXXlBXV5e+/OUvp+PHAQDGobQMoPvuu0/nzp3TU089paamJv3O7/yOdu3a9ZEXJgAAbl5pS0LYsGGDNmzYMOK/P3ny5NhvfHLeHJfON4u6vZ03jblv/nTeeFdQUGD1dt/s5ryBzX2jYzrfpOe8udB9o7D75l/nzaXu71SdevfNiM477d03fzqpJu6bP93jdPq7a0lnyoLzthfnGuzr64tVF/xVcACAmxMDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEETaoniu18DAQOxojmt97vj/50amOJxoEMmLzXBjZFpaWmLXzp492+pdUlJi1ceN5ZCkrq4uq7cT9eJE60heRJEb3eJeh05/J4ZJki5evBi71rmvub3dT0M+d+5c7Nqenh6rd1FRkVXv3D9TqZTV26l3o3iceidCKG58EM+AAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEGM2Sy4zs7O2HlC2dnZsfs6+V6Sl/EUd72XxM26k/yMNCc/ys0O6+josOqdc+hmdjny8vKseic7zu2dk5Nj1TuZak72nuTnDDqc/DA3I805Trd3Ou/L7lqcvDY3j9Kpd3IAyYIDAIxpDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQYzaKp7W1NXbEjhOZ4saUTJ4c/xS5kTa5ubmxa51IE8mLBmlra7N6u9EtToSHe5zOOXSuE8nfT4cb9dLb2xu71o16cc6L29uJkHJjZJzryt17J/pI8s6Lew7TuffO44RzvuPW8gwIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEMSYzYLr7OyMnQXn5Bm5WXA5OTmxa53cOElKJpOxa0tKSqzeiUQidq2bk+Wcb8nLPXNz5px6N3/NyTFrb2+3ertZY04Ol5th5+zn2bNnrd4nT56MXdvY2Gj1dtbt3jd7enrSthYn203yrxWHc16c2rj3S54BAQCCGPUB9M1vflMZGRnDbvPnzx/tHwMAGOfS8k9wn/rUp/T222//3w8xn/4CACa+tEyGyZMnq6KiIh2tAQATRFp+B3T06FFVVlZqzpw5+tKXvqQTJ05csTaVSqm9vX3YDQAw8Y36AFq6dKm2bdumXbt2acuWLWpsbNRnP/tZdXR0XLa+rq5OyWRy6FZVVTXaSwIAjEGjPoDWrFmjP/qjP9KiRYu0atUq/dM//ZNaW1v1k5/85LL1mzZtUltb29DNedkmAGD8SvurA4qLi/WJT3xCx44du+z3E4mE9Z4VAMDEkPb3AXV2dur48eOaPn16un8UAGAcGfUB9JWvfEX19fX6zW9+o3/913/V5z//eWVmZuoLX/jCaP8oAMA4Nur/BHfq1Cl94QtfUEtLi6ZNm6bPfOYz2r9/v6ZNm2b1cWJT3GgLRzpjMAYHB2PX5uXlWb2deJ24kUeXZGVlWfVOjIwbgdLZ2Rm71o3icfbejTNyI4dyc3Nj17r748RTnT592ur9/vvvp613fn5+7NopU6ZYvV3ONe5GWTn1zmOK5F23zvs5465j1AfQjh07RrslAGACIgsOABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABBE2j+OYaQGBgY0aVK8+ehkWbkf/eDkh7k5Zt3d3bFrCwoKrN5O1pibS5aTk2PVO/lUbpaVk5Pl5rU5+V5jiXsOnSxF9xOLnaw+N3cxnXmHbr1zH3KP03lccfMO4z7GpquWZ0AAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCDGbBRPKpWKHSnixH24UTxO/IQbgeJE8ThxKW7vCxcuWL3z8/PTVu9GoDj76a47mUzGri0qKrJ6u9ehE/XixBNJ3rXlXuOlpaWxa8vLy63eM2bMiF3rnm/3/tbS0pK23k4Uj7v37v1ttPEMCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABDEmM2CGxgYUEZGRqxaJwsuKyvLXkdcmZmZVu+LFy/GrnUy6SSpv78/dm1HR4fV28mmctcybdo0q7eTwTZlypS09c7NzbV6u1KpVOxa57qSvCzAvr4+q7eTp1dWVmb1/vjHPx671s1Ia21tteqd85KTk2P1djIM3eN01uLk6cV97OYZEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACCIMZsFl0qlYucaOTlMbu6Zw8lKcg0ODlr1TiaUm4/n5tJNnhz/MnNyrySpuLg4dm1hYaHVO5376ebpOXlt7e3tVu/Tp0+nrXdBQUHsWnd/nHr3mnXz2pxrxb2uuru7Y9c6uYtS/Mw2V9zsQp4BAQCCsAfQ3r17dffdd6uyslIZGRl69dVXh30/iiI99dRTmj59unJzc1VTU6OjR4+O1noBABOEPYC6urq0ePFibd68+bLff/755/W9731PL774og4cOKD8/HytWrVKvb29171YAMDEYf8OaM2aNVqzZs1lvxdFkV544QV9/etf19q1ayVJP/zhD1VeXq5XX31V999///WtFgAwYYzq74AaGxvV1NSkmpqaoa8lk0ktXbpU+/btu+zfSaVSam9vH3YDAEx8ozqAmpqaJEnl5eXDvl5eXj70vQ+rq6tTMpkculVVVY3mkgAAY1TwV8Ft2rRJbW1tQ7eTJ0+GXhIA4AYY1QFUUVEhSWpubh729ebm5qHvfVgikVBRUdGwGwBg4hvVATR79mxVVFRo9+7dQ19rb2/XgQMHVF1dPZo/CgAwztmvguvs7NSxY8eG/tzY2KjDhw+rpKREM2fO1OOPP66//uu/1q233qrZs2frG9/4hiorK3XPPfeM5roBAOOcPYAOHjyoz33uc0N/3rhxoyRp/fr12rZtm5588kl1dXXp4YcfVmtrqz7zmc9o165ddrRFFEWKoihWrRNTc/HiRWsdTrRFZmam1duJB3GjeOKeO8lftxN/I0klJSWxa6dMmWL1zsvLi12bm5tr9Xb2p7Oz0+p95swZq/7UqVOxa1tbW63eTr17rTjn3L1vOhE1TiSQJPtXAc51+7GPfczq7cRque+3dHq3tLTEro0bNWUPoOXLl1/1wS0jI0PPPvusnn32Wbc1AOAmEvxVcACAmxMDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEIQdxXOjTJ48WZMnx1uekzPnZkLl5+fHrnWywyQv+8rNeHKy4OKe50vcXL/S0tLYtdOmTbN6O5ld7rqdHEA3f+3ChQtW/blz52LXup8q3NHREbs2nR+Xks77j7OXI+Fk3jnZiG5v9zidc/ib3/wmdm1XV1esOp4BAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCGLNRPJmZmcrMzIxV60TJZGVljXRJ1zQ4OGjVOzEYTrSOWx/3PF+Sl5dn1SeTydi1btSLE1OSnZ1t9XbOi7MOyY9jcSJW3P1xJBIJq96JsnKuE8nbz56eHqt33CiZS5z9ycjIsHo7x+k+BjmPE07vuLU8AwIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEMWaz4BxOppqT2eRyMunSzcnsKigosHo7+V6Sl2Xl5mRNmhT//6HSmZPlZsFVVlZa9VOnTo1d29raavV2uJlqzv6kM2euu7vb6p1KpdJW/8EHH1i9+/r6Yte617iTd9jb2xu7Nu754BkQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACCIsZMd8yG5ubnKysqKVVtUVBS7rxsjk5eXF7s2JyfH6j0wMBC71o0GcWJNpk2bZvV2z6ETD+LEfUheXI4bU+LENjl7KfmxTYWFhbFrnXMiedd4V1eX1du5bt24HOf+5kZwufsT97FK8qLDJKmjoyN2rROtI3nXlROpFfcYeQYEAAiCAQQACMIeQHv37tXdd9+tyspKZWRk6NVXXx32/QceeEAZGRnDbqtXrx6t9QIAJgh7AHV1dWnx4sXavHnzFWtWr16tM2fODN1efvnl61okAGDisV+EsGbNGq1Zs+aqNYlEQhUVFSNeFABg4kvL74D27NmjsrIyzZs3T48++qhaWlquWJtKpdTe3j7sBgCY+EZ9AK1evVo//OEPtXv3bv3t3/6t6uvrtWbNmiu+TLWurk7JZHLoVlVVNdpLAgCMQaP+PqD7779/6L8XLlyoRYsWae7cudqzZ49WrFjxkfpNmzZp48aNQ39ub29nCAHATSDtL8OeM2eOSktLdezYsct+P5FIqKioaNgNADDxpX0AnTp1Si0tLZo+fXq6fxQAYByx/wmus7Nz2LOZxsZGHT58WCUlJSopKdEzzzyjdevWqaKiQsePH9eTTz6pW265RatWrRrVhQMAxjd7AB08eFCf+9znhv586fc369ev15YtW3TkyBH9wz/8g1pbW1VZWamVK1fqr/7qr6xsMkmaMWNG7L9TWVkZu6+TfSR5OXMFBQVWbyf37Pz581bvnp6e2LXu3ri5dBcuXIhd66xb8rKv+vr6rN7Ocbr5XhkZGVa9kzXm5p45WWOdnZ1W70mT4v8jy9mzZ63ezn66e+9y8vScc+LWu49vZWVlaekdd832AFq+fPlVww7ffPNNtyUA4CZEFhwAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIIhR/zyg0bJw4ULl5ubGqi0tLY3d18lskqT8/Py09b7aJ8V+2H/9139ZvY8ePRq71v0UWjcLLu4+SlJOTo7V26m/WoTU5TjH2d3dbfV2M++cLDM3C865Dp38QklX/CDKy3EyAyVvf5xjlPz9dHIg3cxI5zHI3R8n69I533FreQYEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAhizEbxzJ07N3YExa233hq77+TJ3iFnZmamrXdWVlbs2sbGRqu3Ez3iRqA450Ty4nKccyJJyWQydq0TCSR5MTJtbW1W7/Pnz1v1Tn8ntkeSurq6Yte6e+/sZ2trq9XbiVbq6Oiwen/wwQdWvbOfTvyNW+/speSdQ6c2btQUz4AAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQYzZLLiSkhIVFBTEqi0vL4/d9+LFi9Y63HqHkzXmZlM1NTXFrnWz4CZN8v6/xakfHBy0epeWlsaunTJlitXbWYubY9bc3GzVt7e3x67t7e21ejvXYXFxsdXbuf+49zVn3W4OoJN7JkmpVCp2rXv/cXq7mYTOWpysy7jXIM+AAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBjNkonoyMDGVkZIx63/7+fqu+p6cndq0bJeLE67S0tFi9nWiYzs5Oq7cbl9PV1ZW2tZw/fz52rRPb4+ro6LDqz507Z9U757Cvr8/qHTfySpIqKiqs3oWFhbFrE4mE1dvhHKPkxzY53CgeZ++duBxJ1mNsOmp5BgQACMIaQHV1dbr99ttVWFiosrIy3XPPPWpoaBhW09vbq9raWk2dOlUFBQVat26dHbwIAJj4rAFUX1+v2tpa7d+/X2+99Zb6+/u1cuXKYU8Rn3jiCb3++ut65ZVXVF9fr9OnT+vee+8d9YUDAMY36x8Md+3aNezP27ZtU1lZmQ4dOqRly5apra1NL730krZv36677rpLkrR161Z98pOf1P79+/XpT3969FYOABjXrut3QJc+e6KkpESSdOjQIfX396umpmaoZv78+Zo5c6b27dt32R6pVErt7e3DbgCAiW/EA2hwcFCPP/647rzzTi1YsEDSbz8ELTs7+yMfWlVeXn7FD0irq6tTMpkculVVVY10SQCAcWTEA6i2tlbvvfeeduzYcV0L2LRpk9ra2oZuJ0+evK5+AIDxYUTvA9qwYYPeeOMN7d27VzNmzBj6ekVFhfr6+tTa2jrsWVBzc/MV3z+QSCTS+vp/AMDYZD0DiqJIGzZs0M6dO/XOO+9o9uzZw76/ZMkSZWVlaffu3UNfa2ho0IkTJ1RdXT06KwYATAjWM6Da2lpt375dr732mgoLC4d+r5NMJpWbm6tkMqkHH3xQGzduVElJiYqKivTYY4+purqaV8ABAIaxBtCWLVskScuXLx/29a1bt+qBBx6QJH3nO9/RpEmTtG7dOqVSKa1atUo/+MEPRmWxAICJwxpAURRdsyYnJ0ebN2/W5s2bR7wo6be5anGz1Zx8Nzcnq7u7O3atk9kkeVlwcc79/5dMJmPX9vb2Wr3d3DPnON0sOGc/nVw/ycvscq4TyT+HAwMDVr3D+R3sh1/hei1Tp06NXVtUVGT1ds5JTk6O1dvNgnPW7mbBOUkybk6jsxanNu46yIIDAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAAQxoo9juBEGBgZiR23EjeyR/KgKp7cb9eL0zs/Pt3pXVlbGrs3OzrZ6uzEyTtTLhQsXrN5OHIsbZ+REFLlxRm4cS15eXuxa9+NNnLicK32sypU4HzDpxt840VdZWVlW78mTvYdGZ3/ctaRSqdi1btSYc/9xauNe3zwDAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAAQxZrPgLl68GDsrLZ15YJmZmbFr3XyvjIyMtPXOycmJXVtcXGz1LigosOqTyWTs2vPnz1u929raYtf29/dbvZ3MO+c6kbz9kbyctHTuT25urtXbuW92d3enrbe7P262X2dnZ+xaN3vROeduDqCTM+ccY1w8AwIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABDFmo3gGBgZiR204EStuHEtfX1/s2vb2dqu3U+9G1Di93dgRN0rEifpx4okkL6bEidaRvJiSvLw8q7cbmeLE67hRPJMnx38YcM/hqVOn0rIOyTvn7jXrRl+1tLTErnX3vqSkJHatE6skSYWFhbFrncfOwcHBWHU8AwIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEMWaz4Hp7e5WZmRmr1sk9c7OsPvjgg9i1x44ds3o3NjbGrv2f//kfq/e5c+di13Z3d1u93by2/Pz82LVuBldWVlbsWjcjzdl7N8fMXUs68/QuXrwYu9bNJLxw4ULsWnfvnYw0N38tiiKrPm72meRfK5WVlbFr58yZY/V2zmE68AwIABCENYDq6up0++23q7CwUGVlZbrnnnvU0NAwrGb58uXKyMgYdnvkkUdGddEAgPHPGkD19fWqra3V/v379dZbb6m/v18rV65UV1fXsLqHHnpIZ86cGbo9//zzo7poAMD4Z/1j5K5du4b9edu2bSorK9OhQ4e0bNmyoa/n5eWpoqJidFYIAJiQrut3QG1tbZI++ousH/3oRyotLdWCBQu0adOmq/6SO5VKqb29fdgNADDxjfhVcIODg3r88cd15513asGCBUNf/+IXv6hZs2apsrJSR44c0Ve/+lU1NDTopz/96WX71NXV6ZlnnhnpMgAA49SIB1Btba3ee+89/eIXvxj29YcffnjovxcuXKjp06drxYoVOn78uObOnfuRPps2bdLGjRuH/tze3q6qqqqRLgsAME6MaABt2LBBb7zxhvbu3asZM2ZctXbp0qWSfvsemcsNoEQiYb9GHwAw/lkDKIoiPfbYY9q5c6f27Nmj2bNnX/PvHD58WJI0ffr0ES0QADAxWQOotrZW27dv12uvvabCwkI1NTVJkpLJpHJzc3X8+HFt375df/iHf6ipU6fqyJEjeuKJJ7Rs2TItWrQoLQcAABifrAG0ZcsWSb99s+n/t3XrVj3wwAPKzs7W22+/rRdeeEFdXV2qqqrSunXr9PWvf33UFgwAmBjsf4K7mqqqKtXX11/Xgi7p6OjQwMBArNrm5ubYfVtaWqx1nDlzJnbtr3/967T1vvSS97j6+vpi16ZSKat3f3+/Ve/karnZVHl5ebFr42YLXuJkk7n5Xjk5OVa9kx3n7L2kj7yR/Grc69A55+45ceqdY5T8+4TT370Onay+ZDJp9XbW4lxXcWvJggMABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABDHizwNKt7a2tthxDk5UxYULF6x1nD59Om29Ozo6YtdmZGRYvZ3oFjcCpaenx6rPzs6OXetGiTj1nZ2dVm/nvAwODlq93TiW3Nzc2LVOhJDkRSu5ETVORJFzjJK3P85jhCT7k5nd+77DOU7n8UryzotzXfX29sbrGbsjAACjiAEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAhizGbBtba2xs4TcjK+uru7rXU4WUn5+flp6x03F+8SJ7fJ7d3W1mbVOzlpbuadkzPn5ONJUllZWexa9xw665a8tRcXF1u9S0pKYte2tLRYvZ3z4uTGSVJWVlbsWvd8p/Mad3Pp4j4OSv7+OPc35/EtbmYgz4AAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEGM2Sievr6+2HEyURTF7utE1EheBEpmZqbV24kS6ejosHr39/fHrnViRCQ/LsdZixuVFDfyQ/L3Z+rUqbFrnWtQ8mNncnJy0lLr1peWllq9e3p6Yte6e+/E67jXrBNPJHnXuHtfHhgYiF37wQcfWL2d69B57CSKBwAwpjGAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBMIAAAEEwgAAAQTCAAABBjNksuMmTJ8fOKXIyvtxMKCcnzcl2k6Tc3NzYtW5OlpPB1dvba/V2s+McFy9etOqdXC0nO0zysq/c3nl5eVa9c62412EymYxdm0gkrN7O/a2trc3q3dnZGbvWvcbz8/Ot+mnTpsWudffeyRl0cuMk73HCyV3s6+uLVcczIABAENYA2rJlixYtWqSioiIVFRWpurpaP/vZz4a+39vbq9raWk2dOlUFBQVat26dmpubR33RAIDxzxpAM2bM0HPPPadDhw7p4MGDuuuuu7R27Vq9//77kqQnnnhCr7/+ul555RXV19fr9OnTuvfee9OycADA+Gb9Dujuu+8e9ue/+Zu/0ZYtW7R//37NmDFDL730krZv36677rpLkrR161Z98pOf1P79+/XpT3969FYNABj3Rvw7oIGBAe3YsUNdXV2qrq7WoUOH1N/fr5qamqGa+fPna+bMmdq3b98V+6RSKbW3tw+7AQAmPnsA/fKXv1RBQYESiYQeeeQR7dy5U7fddpuampqUnZ2t4uLiYfXl5eVqamq6Yr+6ujolk8mhW1VVlX0QAIDxxx5A8+bN0+HDh3XgwAE9+uijWr9+vX71q1+NeAGbNm1SW1vb0O3kyZMj7gUAGD/s9wFlZ2frlltukSQtWbJE//7v/67vfve7uu+++9TX16fW1tZhz4Kam5tVUVFxxX6JRMJ+bwEAYPy77vcBDQ4OKpVKacmSJcrKytLu3buHvtfQ0KATJ06ourr6en8MAGCCsZ4Bbdq0SWvWrNHMmTPV0dGh7du3a8+ePXrzzTeVTCb14IMPauPGjSopKVFRUZEee+wxVVdX8wo4AMBHWAPo7Nmz+uM//mOdOXNGyWRSixYt0ptvvqk/+IM/kCR95zvf0aRJk7Ru3TqlUimtWrVKP/jBD0a0sCiKrAiKuNyoCicaxu3tHJ8TNyQpdoyR5MeO5OTkWPUO959jndgZN+bH6e2u243ucfT391v1TsRKOuOmCgoKrN7pjIRy72/OfcjdHydGyD0nznXr7H3cdVgD6KWXXrrq93NycrR582Zt3rzZaQsAuAmRBQcACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAjCTsNOt0vxNE78hBOx4sax9PX1xa5NZwSKsw633l13OiNQMjIyrHonzsjde6f3pEne/8u59W40jMOJWHEinlzOfV7y7j9OreTfJ5wYLre3c19275vO/W0kj1fXug9lROkIXLsOp06d4kPpAGACOHnypGbMmHHF74+5ATQ4OKjTp0+rsLBw2HRub29XVVWVTp48qaKiooArTC+Oc+K4GY5R4jgnmtE4ziiK1NHRocrKyqs+2x9z/wQ3adKkq07MoqKiCb35l3CcE8fNcIwSxznRXO9xJpPJa9bwIgQAQBAMIABAEONmACUSCT399NP2B3+NNxznxHEzHKPEcU40N/I4x9yLEAAAN4dx8wwIADCxMIAAAEEwgAAAQTCAAABBjJsBtHnzZn384x9XTk6Oli5dqn/7t38LvaRR9c1vflMZGRnDbvPnzw+9rOuyd+9e3X333aqsrFRGRoZeffXVYd+PokhPPfWUpk+frtzcXNXU1Ojo0aNhFnsdrnWcDzzwwEf2dvXq1WEWO0J1dXW6/fbbVVhYqLKyMt1zzz1qaGgYVtPb26va2lpNnTpVBQUFWrdunZqbmwOteGTiHOfy5cs/sp+PPPJIoBWPzJYtW7Ro0aKhN5tWV1frZz/72dD3b9RejosB9OMf/1gbN27U008/rf/4j//Q4sWLtWrVKp09ezb00kbVpz71KZ05c2bo9otf/CL0kq5LV1eXFi9erM2bN1/2+88//7y+973v6cUXX9SBAweUn5+vVatW2aGUoV3rOCVp9erVw/b25ZdfvoErvH719fWqra3V/v379dZbb6m/v18rV65UV1fXUM0TTzyh119/Xa+88orq6+t1+vRp3XvvvQFX7YtznJL00EMPDdvP559/PtCKR2bGjBl67rnndOjQIR08eFB33XWX1q5dq/fff1/SDdzLaBy44447otra2qE/DwwMRJWVlVFdXV3AVY2up59+Olq8eHHoZaSNpGjnzp1Dfx4cHIwqKiqib33rW0Nfa21tjRKJRPTyyy8HWOHo+PBxRlEUrV+/Plq7dm2Q9aTL2bNnI0lRfX19FEW/3busrKzolVdeGar5z//8z0hStG/fvlDLvG4fPs4oiqLf//3fj/7sz/4s3KLSZMqUKdHf/d3f3dC9HPPPgPr6+nTo0CHV1NQMfW3SpEmqqanRvn37Aq5s9B09elSVlZWaM2eOvvSlL+nEiROhl5Q2jY2NampqGravyWRSS5cunXD7Kkl79uxRWVmZ5s2bp0cffVQtLS2hl3Rd2traJEklJSWSpEOHDqm/v3/Yfs6fP18zZ84c1/v54eO85Ec/+pFKS0u1YMECbdq0Sd3d3SGWNyoGBga0Y8cOdXV1qbq6+obu5ZgLI/2w8+fPa2BgQOXl5cO+Xl5erl//+teBVjX6li5dqm3btmnevHk6c+aMnnnmGX32s5/Ve++9p8LCwtDLG3VNTU2SdNl9vfS9iWL16tW69957NXv2bB0/flx/+Zd/qTVr1mjfvn1p/ZyfdBkcHNTjjz+uO++8UwsWLJD02/3Mzs5WcXHxsNrxvJ+XO05J+uIXv6hZs2apsrJSR44c0Ve/+lU1NDTopz/9acDV+n75y1+qurpavb29Kigo0M6dO3Xbbbfp8OHDN2wvx/wAulmsWbNm6L8XLVqkpUuXatasWfrJT36iBx98MODKcL3uv//+of9euHChFi1apLlz52rPnj1asWJFwJWNTG1trd57771x/zvKa7nScT788MND/71w4UJNnz5dK1as0PHjxzV37twbvcwRmzdvng4fPqy2tjb94z/+o9avX6/6+vobuoYx/09wpaWlyszM/MgrMJqbm1VRURFoVelXXFysT3ziEzp27FjopaTFpb272fZVkubMmaPS0tJxubcbNmzQG2+8oZ///OfDPjaloqJCfX19am1tHVY/XvfzSsd5OUuXLpWkcbef2dnZuuWWW7RkyRLV1dVp8eLF+u53v3tD93LMD6Ds7GwtWbJEu3fvHvra4OCgdu/ererq6oArS6/Ozk4dP35c06dPD72UtJg9e7YqKiqG7Wt7e7sOHDgwofdV+u2n/ra0tIyrvY2iSBs2bNDOnTv1zjvvaPbs2cO+v2TJEmVlZQ3bz4aGBp04cWJc7ee1jvNyDh8+LEnjaj8vZ3BwUKlU6sbu5ai+pCFNduzYESUSiWjbtm3Rr371q+jhhx+OiouLo6amptBLGzV//ud/Hu3ZsydqbGyM/uVf/iWqqamJSktLo7Nnz4Ze2oh1dHRE7777bvTuu+9GkqJvf/vb0bvvvhv993//dxRFUfTcc89FxcXF0WuvvRYdOXIkWrt2bTR79uyop6cn8Mo9VzvOjo6O6Ctf+Uq0b9++qLGxMXr77bej3/3d341uvfXWqLe3N/TSY3v00UejZDIZ7dmzJzpz5szQrbu7e6jmkUceiWbOnBm988470cGDB6Pq6uqouro64Kp91zrOY8eORc8++2x08ODBqLGxMXrttdeiOXPmRMuWLQu8cs/Xvva1qL6+PmpsbIyOHDkSfe1rX4syMjKif/7nf46i6Mbt5bgYQFEURd///vejmTNnRtnZ2dEdd9wR7d+/P/SSRtV9990XTZ8+PcrOzo4+9rGPRffdd1907Nix0Mu6Lj//+c8jSR+5rV+/Poqi374U+xvf+EZUXl4eJRKJaMWKFVFDQ0PYRY/A1Y6zu7s7WrlyZTRt2rQoKysrmjVrVvTQQw+Nu/95utzxSYq2bt06VNPT0xP96Z/+aTRlypQoLy8v+vznPx+dOXMm3KJH4FrHeeLEiWjZsmVRSUlJlEgkoltuuSX6i7/4i6itrS3swk1/8id/Es2aNSvKzs6Opk2bFq1YsWJo+ETRjdtLPo4BABDEmP8dEABgYmIAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIL4X+mhg38IuSJqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                  [-1.0, 0.0, 1.0],\n",
    "                                  [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination with Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # ... <1>\n",
    "    nn.Linear(8 * 8 * 8, 32),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network with Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) # without instantiate in init method\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2) # without instantiate in init method\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0411, -0.0358]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-19 04:27:55.454767 Epoch 1, Training loss 0.5804177875731401\n",
      "2023-01-19 04:28:21.412502 Epoch 10, Training loss 0.322604932223156\n",
      "2023-01-19 04:28:49.941205 Epoch 20, Training loss 0.290214629879423\n",
      "2023-01-19 04:29:19.658806 Epoch 30, Training loss 0.2664595464612268\n",
      "2023-01-19 04:29:49.138860 Epoch 40, Training loss 0.24809699235068763\n",
      "2023-01-19 04:30:22.040087 Epoch 50, Training loss 0.23048108080579977\n",
      "2023-01-19 04:30:54.535498 Epoch 60, Training loss 0.21473691987383897\n",
      "2023-01-19 04:31:23.566669 Epoch 70, Training loss 0.20075724254937688\n",
      "2023-01-19 04:31:52.686164 Epoch 80, Training loss 0.1832931741692458\n",
      "2023-01-19 04:32:21.003606 Epoch 90, Training loss 0.16998054533247736\n",
      "2023-01-19 04:32:49.606134 Epoch 100, Training loss 0.15532025080292847\n"
     ]
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad(): # not update parameter\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # give index highest value output\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "            \n",
    "            print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Computation to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt', map_location=device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Memory Capacity with Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalize Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way to stabilize generalization is to add a regularization term to the loss. This\n",
    "term is crafted so that the weights of the model tend to be small on their own, limiting\n",
    "how much training makes them grow. In other words, it is a penalty on larger weight\n",
    "values. This makes the loss have a smoother topography, and thereâ€™s relatively less to\n",
    "gain from fitting individual samples.\n",
    "\n",
    "The most popular regularization terms of this kind are L2 regularization, which is\n",
    "the sum of squares of all weights in the model, and L1 regularization, which is the sum\n",
    "of the absolute values of all weights in the model. Both of them are scaled by a\n",
    "(small) factor, which is a hyperparameter we set prior to training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization is also referred to as weight decay. The reason for this name is that,\n",
    "thinking about SGD and backpropagation, the negative gradient of the L2 regularization term with respect to a parameter `w_i` is - `2 * lambda * w_i`, where lambda is the aforementioned hyperparameter, simply named weight decay in PyTorch. So, adding L2 regularization to the loss function is equivalent to decreasing each weight by an amount proportional to its current value during the optimization step (hence, the\n",
    "name weight decay). Note that weight decay applies to all parameters of the network, such as biases.\n",
    "\n",
    "In PyTorch, we could implement regularization pretty easily by adding a term to the loss. After computing the loss, whatever the loss function is, we can iterate the parameters of the model, sum their respective square (for L2) or abs (for L1), and backpropagate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2_reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters()) # Replaces pow(2.0) with abs() for L1 regularization\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the SGD optimizer in PyTorch already has a `weight_decay` parameter that corresponds to `2 * lambda`, and it directly performs weight decay during the update as described previously. It is fully equivalent to adding the L2 norm of weights to the loss, without the need for accumulating terms in the loss and involving autograd."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An effective strategy for combating overfitting was originally proposed in 2014 by Nitish Srivastava and coauthors from Geoff Hintonâ€™s group in Toronto, in a paper aptly entitled â€œDropout: a Simple Way to Prevent Neural Networks from Overfittingâ€ (http://mng.bz/nPMa). The idea behind dropout is indeed simple: zero out a random fraction of outputs from neurons across the network, where the randomization happens at each training iteration.\n",
    "\n",
    "This procedure effectively generates slightly different models with different neuron topologies at each iteration, giving neurons in the model less chance to coordinate in the memorization process that happens during overfitting. An alternative point of view is that dropout perturbs the features being generated by the model, exerting an effect that is close to augmentation, but this time throughout the network.\n",
    "\n",
    "In PyTorch, we can implement dropout in a model by adding an `nn.Dropout` module between the nonlinear activation function and the linear or convolutional module of the subsequent layer. As an argument, we need to specify the probability with which inputs will be zeroed out. In case of convolutions, weâ€™ll use the specialized `nn.Dropout2d` or `nn.Dropout3d`, which zero out entire channels of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout was all the rage when, in 2015, another seminal paper was published by Sergey Ioffe and Christian Szegedy from Google, entitled â€œBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftâ€ (https://arxiv.org/abs/1502.03167). The paper described a technique that had multiple beneficial effects on training: allowing us to increase the learning rate and make training less dependent on initialization and act as a regularizer, thus representing an alternative to dropout.\n",
    "\n",
    "The main idea behind batch normalization is to rescale the inputs to the activations of the network so that minibatches have a certain desirable distribution. Recalling the mechanics of learning and the role of nonlinear activation functions, this helps avoid the inputs to activation functions being too far into the saturated portion of the function, thereby killing gradients and slowing training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical terms, batch normalization shifts and scales an intermediate input using the mean and standard deviation collected at that intermediate location over the samples of the minibatch. The regularization effect is a result of the fact that an individual sample and its downstream activations are always seen by the model as shifted and scaled, depending on the statistics across the randomly extracted minibatch. This is in itself a form of principled augmentation. The authors of the paper suggest that using batch normalization eliminates or at least alleviates the need for dropout.\n",
    "\n",
    "Batch normalization in PyTorch is provided through the `nn.BatchNorm1D`, `nn.BatchNorm2d`, and `nn.BatchNorm3d` modules, depending on the dimensionality of the input. Since the aim for batch normalization is to rescale the inputs of the activations, the natural location is after the linear transformation (convolution, in this case) and the activation, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Connection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original NetDepth without skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        \n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Very Deep Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exceeding 100 layers in a convolutional neural network, The standard strategy is to define a building block, such as a (Conv2d, ReLU, Conv2d) + skip connection block, and then build the network dynamically in a for loop. Letâ€™s see it done in practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a module subclass whose sole job is to provide the computation for one\n",
    "blockâ€”that is, one group of convolutions, activation, and skip connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since weâ€™re planning to generate a deep model, we are including batch normalization in the block, since this will help prevent gradients from vanishing during training. Weâ€™d now like to generate a 100-block network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
